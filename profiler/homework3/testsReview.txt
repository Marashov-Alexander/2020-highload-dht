1) Анализ профилирования:

Alloc test PUT (Homework 2)
При просмотре диаграммы профилирования видно, что около 8 процентов памяти уходит на работу с TCP соединениями, а остальное - на работу селекторов. В селекторах примерно 43 процента потребяемой памяти уходит на работу handleRequest (10 процентов - отправка ответа, 5 - извлечение Required Parameter, 24 процента - на работу put метода DAO).

Alloc test PUT (Homework 3)
Немного другая картина появляется после реализации асинхронности работы селекторов относительно ожидания и отправки ответов. Теперь SelectorThread отвечает лишь за 36 процентов потребляемой памяти и занимается важными непосредственно для него делами: парсит запросы и направляет их в нужный метод. Теперь 63 процента потребляемой памяти принадлежит работе с ExecutorService: через него осуществляются put-запросы в DAO (21 процент) и отправка ответов (16 процентов). 

------
CPU test PUT (Homework 2)
95 процентов процессорного времени потребляет SelectorThread. В этом задании он отвечает не только за выбор соединений: в его контексте происходит взаимодействие с DAO (14%), работа с сессиями (направления запросов, формирование ответов и их отправка - 47%)

CPU test PUT (Homework 3)
Здесь тоже видны существенные изменения: благодаря оптимизации SelectorThread потребляет всего лишь 33 процента процессорного времени, где занимается только выборкой и доставкой запросов до ExecutorService с нужными инструкциями. Вся работа с DAO и формированием ответа происходит в ExecutorService (64.7%)

------
LOCK test PUT (Homework 2)
Как и ожидалось: все блокировки происходят внутри SelectorThread, а именно - Dao.upsert. Из них 50 процентов принадлежат методу непосредственной вставки записей (read locks), а остальные - write locks, берутся при вызове метода flush.

LOCK test PUT (Homework 3)
Профилирование проводилось при значительно большем количестве как потоков, так и соединений, поэтому на диаграмме гораздо больше примеров вхождения (samples) блокировок. Из них 71 процент приходится на ExecutorService - это работа с DAO (12%) и получение задач воркерами из очереди.


============


Alloc test GET (Homework 2)
Абсолютно 100 процентов памяти потребляется в контексте SelectorThread. Из них почти 5% - парсинг запроса, остальное - обработка полученного запроса. Поиск нужного значения в ДАО занимает 38 процентов, при этом большую часть из этого составляет работа с итераторами (в частности, поиск в SSTables). 17 процентов занимает логирование + 29% памяти уходит на метод String.format + 4% - на ByteBuffer.toString(). Разумеется, что это не является полезной нагрузкой и при введении сервера в эксплуатацию должно быть скрыто.

Alloc test GET (Homework 3)
Как и в диаграмме PUT метода HW3, большую часть потребляемой памяти забирает Thread.run для работы ExecutorService, ведь именно в нём теперь происходит обращение к DAO (44%), отправка ответа (5 процентов) и разделение задач между воркерами с помощью очереди (1.47%). В SelectorThread'е остаётся лишь select (2%), handleParsedRequest (9%), parseRequest (18%) и прочие вспомогательные методы. 

------
CPU test GET (Homework 2)
Вся работа процессора снова идёт в SelectorThread: отправка запроса (12%) и обработка get метода (80%). В свою очередь DAO большую часть процессорного времени проводит в итерировании по данным: чтение sstables, создание итераторов и их объединение, двоичный поиск по значениям (59%). Не обошлось и без логирования: на него уходило 11 + 5 процентов процессорного времени.


CPU test GET (Homework 3)
10 процентов процессорного времени - на получение воркерами задач, 13% - на работу с ДАО, 43 - на отправку ответа. Разгруженный selectorThread теперь занимает лишь 30 процентов времени процессора.

------
LOCK test GET (Homework 2)
Видимо, при чтении блокировка таблиц не потребовалась

LOCK test GET (Homework 3)
Большая часть блокировок связана с работой BlockingQueue в реализации ExecutorService.


2) Анализ результатов нагрузочного тестирования сервера с помощью wrk2:
При сравнении результатов нагрузочного тестирования DAO сервиса второго и третьего этапов стало понятно, что введение асинхронной (относительно ожидания и отправки ответов) работы селекторов позволила значительно улучшить показатели скорости ответа сервера как на put запросы, так и на get.
Для сравнения:
100% put запросов у реализации второго этапа обслуживалось не более чем за 13.66 мс, а у третьего этапа - 9.02 мс.
100% get запросов у реализации второго этапа обслуживалось не более чем за 11.51 мс, в то время как на третьем этапе сервис обработал всё не более чем за 8.82 мс.
Стоит заметить, что при этом на втором этапе количество потоков было равно 5 (против 10 на третьем этапе), а количество соединений - 20 (против 64 на третьем этапе).
